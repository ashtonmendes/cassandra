= Data Definition

CQL stores data in _tables_, whose schema defines the layout of said
data in the table, and those tables are grouped in _keyspaces_. A
keyspace defines a number of options that applies to all the tables it
contains, most prominently of which is the
`replication strategy <replication-strategy>` used by the keyspace. It
is generally encouraged to use one keyspace by _application_, and thus
many cluster may define only one keyspace.

This section describes the statements used to create, modify, and remove
those keyspace and tables.

== Common definitions

The names of the keyspaces and tables are defined by the following
grammar:

[source,bnf]
----
include::example$BNF/ks_table.bnf[]
----

Both keyspace and table name should be comprised of only alphanumeric
characters, cannot be empty and are limited in size to 48 characters
(that limit exists mostly to avoid filenames (which may include the
keyspace and table name) to go over the limits of certain file systems).
By default, keyspace and table names are case-insensitive (`myTable` is
equivalent to `mytable`) but case sensitivity can be forced by using
double-quotes (`"myTable"` is different from `mytable`).

Further, a table is always part of a keyspace and a table name can be
provided fully-qualified by the keyspace it is part of. If is is not
fully-qualified, the table is assumed to be in the _current_ keyspace
(see `USE statement <use-statement>`).

Further, the valid names for columns is simply defined as:

[source,bnf]
----
include::example$BNF/column.bnf[]
----

We also define the notion of statement options for use in the following
section:

[source,bnf]
----
include::example$BNF/options.bnf[]
----

[[create-keyspace-statement]]
== CREATE KEYSPACE

A keyspace is created using a `CREATE KEYSPACE` statement:

[source,bnf]
----
include::example$BNF/create_ks.bnf[]
----

For instance:

[source,cql]
----
include::example$CQL/create_ks.cql[]
----

Attempting to create a keyspace that already exists will return an error
unless the `IF NOT EXISTS` option is used. If it is used, the statement
will be a no-op if the keyspace already exists.

The supported `options` are:

[cols=",,,,",options="header",]
|===
|name |kind |mandatory |default |description
|`replication` | _map_ | yes | The replication strategy and options to use for the keyspace (see
details below).
|`durable_writes` | _simple_ | no | true | Whether to use the commit log for updates on this keyspace (disable this
option at your own risk!).
|===

The `replication` property is mandatory and must at least contains the
`'class'` sub-option which defines the
`replication strategy <replication-strategy>` class to use. The rest of
the sub-options depends on what replication strategy is used. By
default, Cassandra support the following `'class'`:

[[replication-strategy]]
=== `SimpleStrategy`

A simple strategy that defines a replication factor for data to be
spread across the entire cluster. This is generally not a wise choice
for production because it does not respect datacenter layouts and can
lead to wildly varying query latency. For a production ready strategy,
see `NetworkTopologyStrategy`. `SimpleStrategy` supports a single
mandatory argument:

[cols=",,,",options="header",]
|===
|sub-option |type |since |description
|`'replication_factor'` | int | all | The number of replicas to store per range
|===

=== `NetworkTopologyStrategy`

A production ready replication strategy that allows to set the
replication factor independently for each data-center. The rest of the
sub-options are key-value pairs where a key is a data-center name and
its value is the associated replication factor. Options:

[cols=",,,",options="header",]
|===
|sub-option |type |since |description
|`'<datacenter>'` | int | all | The number of replicas to store per range in the provided datacenter.
|`'replication_factor'` | int | 4.0 | The number of replicas to use as a default per datacenter if not
specifically provided. Note that this always defers to existing
definitions or explicit datacenter settings. For example, to have three
replicas per datacenter, supply this with a value of 3.
|===

Note that when altering keyspaces and supplying `replication_factor`,
auto-expansion will only _add_ new datacenters for safety, it will not
alter existing datacenters or remove any even if they are no longer in
the cluster. If you want to remove datacenters while still supplying
`replication_factor`, explicitly zero out the datacenter you want to
have zero replicas.

An example of auto-expanding datacenters with two datacenters: `DC1` and
`DC2`:

[source,cql]
----
include::example$CQL/autoexpand_ks.cql[]
----
will result in:
[source,plaintext]
----
include::example$RESULTS/autoexpand_ks.result[]
----

An example of auto-expanding and overriding a datacenter:

[source,cql]
----
include::example$CQL/autoexpand_ks_override.cql[]
----
will result in:
[source,plaintext]
----
include::example$RESULTS/autoexpand_ks_override.result[]
----

An example that excludes a datacenter while using `replication_factor`:

[source,cql]
----
include::example$CQL/autoexpand_exclude_dc.cql[]
----
will result in:
[source,plaintext]
----
include::example$RESULTS/autoexpand_exclude_dc.result[]
----

If transient replication has been enabled, transient replicas can be
configured for both `SimpleStrategy` and `NetworkTopologyStrategy` by
defining replication factors in the format
`'<total_replicas>/<transient_replicas>'`

For instance, this keyspace will have 3 replicas in DC1, 1 of which is
transient, and 5 replicas in DC2, 2 of which are transient:

[source,cql]
----
include::example$CQL/create_ks_trans_repl.cql[]
----

[[use-statement]]
== USE

The `USE` statement allows to change the _current_ keyspace (for the
_connection_ on which it is executed). A number of objects in CQL are
bound to a keyspace (tables, user-defined types, functions, ...) and the
current keyspace is the default keyspace used when those objects are
referred without a fully-qualified name (that is, without being prefixed
a keyspace name). A `USE` statement simply takes the keyspace to use as
current as argument:

[source,bnf]
----
include::example$BNF/use_ks.bnf[]
----
Using CQL:
[source,cql]
----
include::example$CQL/use_ks.cql[]
----

[[alter-keyspace-statement]]
== ALTER KEYSPACE

An `ALTER KEYSPACE` statement allows to modify the options of a
keyspace:

[source,bnf]
----
include::example$BNF/alter_ks.bnf[]
----

For instance:

[source,cql]
----
include::example$CQL/alter_ks.cql[]
----

The supported options are the same than for
`creating a keyspace <create-keyspace-statement>`.

[[drop-keyspace-statement]]
== DROP KEYSPACE

Dropping a keyspace can be done using the `DROP KEYSPACE` statement:

[source,bnf]
----
include::example$BNF/drop_ks.bnf[]
----

For instance:

[source,cql]
----
include::example$CQL/drop_ks.cql[]
----

Dropping a keyspace results in the immediate, irreversible removal of
that keyspace, including all the tables, UTD and functions in it, and
all the data contained in those tables.

If the keyspace does not exists, the statement will return an error,
unless `IF EXISTS` is used in which case the operation is a no-op.

[[create-table-statement]]
== CREATE TABLE

Creating a new table uses the `CREATE TABLE` statement:

[source,bnf]
----
include::example$BNF/create_table.bnf[]
----

For example, here are some CQL statements to create tables:

[source,cql]
----
include::example$CQL/create_table.cql[]
----

A CQL table has a name and is composed of a set of _rows_. Creating a
table amounts to defining which `columns
<column-definition>` the rows will be composed, which of those columns
compose the `primary key <primary-key>`, as well as optional
`options <create-table-options>` for the table.

Attempting to create an already existing table will return an error
unless the `IF NOT EXISTS` directive is used. If it is used, the
statement will be a no-op if the table already exists.

[[column-definition]]
=== Column definitions

Every rows in a CQL table has a set of predefined columns defined at the
time of the table creation (or added later using an
`alter statement<alter-table-statement>`).

A `column_definition` is primarily comprised of the name of the column
defined and it's `type <data-types>`, which restrict which values are
accepted for that column. Additionally, a column definition can have the
following modifiers:

`STATIC`::
  it declares the column as being a `static column <static-columns>`.
`PRIMARY KEY`::
  it declares the column as being the sole component of the
  `primary key <primary-key>` of the table.

==== Static columns

Some columns can be declared as `STATIC` in a table definition. A column
that is static will be “shared” by all the rows belonging to the same
partition (having the same `partition key <partition-key>`). For
instance:

[source,cql]
----
include::example$CQL/create_static_column.cql[]
include::example$CQL/insert_static_data.cql[]
include::example$CQL/select_static_data.cql[]
----
results in:
[source,cql]
----
include::example$RESULTS/select_static_data.result[]
----

As can be seen, the `s` value is the same (`static1`) for both of the
row in the partition (the partition key in that example being `pk`, both
rows are in that same partition): the 2nd insertion has overridden the
value for `s`.

The use of static columns as the following restrictions:

* tables with the `COMPACT STORAGE` option (see below) cannot use them.
* a table without clustering columns cannot have static columns (in a
table without clustering columns, every partition has only one row, and
so every column is inherently static).
* only non `PRIMARY KEY` columns can be static.

[[primary-key]]
=== The Primary key

Within a table, a row is uniquely identified by its `PRIMARY KEY`, and
hence all tables *must* define one PRIMARY KEY. 
A `PRIMARY KEY` is composed of one or more of the defined columns in the table. 
Syntactically, the primary key is defined the keywords `PRIMARY KEY` 
followed by a comma-separated list of the column names within parenthesis.
If the primary key has only one column, you can alternatively add the `PRIMARY KEY` phrase to
that column in the table definition. 
The order of the columns in the primary key definition define the partition key and 
clustering columns.

A CQL primary key is composed of two parts:

`partition key <partition-key>`::
* It is the first component of the primary key definition. 
It can be a single column or, using an additional set of parenthesis, can be multiple columns. 
A table must have at least one partition key, the smallest possible table definition is:
+
[source,cql]
----
include::example$CQL/create_table_single_pk.cql[]
----
`clustering columns <clustering-columns>`::
* The columns are the columns that follow the partition key in the primary key definition.
The order of those columns define the _clustering order_.

Some examples of primary key definition are:

* `PRIMARY KEY (a)`: `a` is the single partition key and there are no clustering columns
* `PRIMARY KEY (a, b, c)` : `a` is the single partition key and `b` and `c` are the clustering columns
* `PRIMARY KEY ((a, b), c)` : `a` and `b` compose the _composite_ partition key and `c` is the clustering column

[IMPORTANT]
====
The primary key uniquely identifies a row in the table, as described above. 
A consequence of this uniqueness is that if another row is inserted using the same primary key, 
then an `UPSERT` occurs and an existing row with the same primary key is replaced. 
Columns that are not part of the primary key cannot define uniqueness.
====

[[partition-key]]
==== Partition key

Within a table, CQL defines the notion of a _partition_ that defines the location of data within a Cassandra cluster.
A partition is the set of rows that share the same value for their partition key. 

Note that if the partition key is composed of multiple columns, then rows belong to the same partition 
when they have the same values for all those partition key columns. 
A hash is computed from the partition key columns and that hash value defines the partition location.
So, for instance, given the following table definition and content:

[source,cql]
----
include::example$CQL/create_table_compound_pk.cql[]
include::example$CQL/insert_table_compound_pk.cql[]
include::example$CQL/select_table_compound_pk.cql[]
----

will result in
[source,cql]
----
include::example$RESULTS/select_table_compound_pk.result[]
----
<1> Rows 1 and 2 are in the same partition, because both columns `a` and `b` are zero.
<2> Rows 3 and 4 are in the same partition, but a different one, because column `a` is zero and column `b` is 1 in both rows.
<3> Row 5 is in a third partition by itself, because both columns `a` and `b` are 1.

Note that a table always has a partition key, and that if the table has
no `clustering columns`, then every partition of that table has a single row.
because the partition key, compound or otherwise, identifies a single location.

The most important property of partition is that all the rows belonging
to the same partition are guaranteed to be stored on the same set of
replica nodes. 
In other words, the partition key of a table defines which rows will be localized on the same 
node in the cluster. 
The localization of data is important to the efficient retrieval of data, requiring the Cassandra coordinator
to contact as few nodes as possible.
However, there is a flip-side to this guarantee, and all rows sharing a partition key will be stored on the same 
node, creating a hotspot for both reading and writing.
While selecting a primary key that groups table rows assists batch updates and can ensure that the updates are 
_atomic_ and done in _isolation_, the partitions must be sized "just right, not too big nor too small".

Data modeling that considers the querying patterns and assigns primary keys based on the queries will have the lowest 
latency in fetching data.

[[clustering-columns]]
==== Clustering columns

The clustering columns of a table define the clustering order for the partition of that table. 
For a given `partition`, all rows are ordered by that clustering order. Clustering columns also add uniqueness to
a row in a table.

For instance, given:

[source,cql]
----
include::example$CQL/create_table_clustercolumn.cql[]
include::example$CQL/insert_table_clustercolumn.cql[]
include::example$CQL/select_table_clustercolumn.cql[]
----

will result in
[source,cql]
----
include::example$RESULTS/select_table_clustercolumn.result[]
----
<1> Row 1 is in one partition, and Rows 2-5 are in a different one. The display order is also different.

Looking more closely at the four rows in the same partition, the `b` clustering column defines the order in which those rows 
are displayed. 
Whereas the partition key of the table groups rows on the same node, the clustering columns control 
how those rows are stored on the node. 

That sorting allows the very efficient retrieval of a range of rows within a partition:

[source,cql]
----
include::example$CQL/select_range.cql[]
----

will result in
[source,cql]
----
include::example$RESULTS/select_range.result[]
----

[[create-table-options]]
=== Table options

A CQL table has a number of options that can be set at creation (and,
for most of them, altered later). These options are specified after the
`WITH` keyword.

Amongst those options, however, are two important ones that cannot be changed after
creation and influence which queries can be done against the table: 
* `COMPACT STORAGE` 
* `CLUSTERING ORDER` 
Those, as well as the other options of a table, are described in the following
sections.

==== Compact tables

[WARNING]
.Warning
====
Since Cassandra 3.0, compact tables have the exact same layout
internally than non compact ones (for the same schema obviously), and
declaring a table compact *only* creates artificial limitations on the
table definition and usage. It only exists for historical reason and is
preserved for backward compatibility And as `COMPACT STORAGE` cannot, as
of Cassandra , be removed, it is strongly discouraged to create new
table with the `COMPACT STORAGE` option.
====

A _compact_ table is one defined with the `COMPACT STORAGE` option.
This option is only maintained for backward compatibility for
definitions created before CQL version 3 and shouldn't be used for new
tables. Declaring a table with this option creates limitations for the
table which are largely arbitrary (and exists for historical reasons).
Amongst those limitation:

* a compact table cannot use collections nor static columns.
* if a compact table has at least one clustering column, then it must
have _exactly_ one column outside of the primary key ones. This imply
you cannot add or remove columns after creation in particular.
* a compact table is limited in the indexes it can create, and no
materialized view can be created on it.

[[clustering-order]]
==== Reversing the clustering order

The clustering order of a table is defined by the
`clustering columns <clustering-columns>` of that table. By default,
that ordering is based on natural order of those clustering order, but
the `CLUSTERING ORDER` allows to change that clustering order to use the
_reverse_ natural order for some (potentially all) of the columns.

The `CLUSTERING ORDER` option takes the comma-separated list of the
clustering column, each with a `ASC` (for _ascendant_, e.g. the natural
order) or `DESC` (for _descendant_, e.g. the reverse natural order).
Note in particular that the default (if the `CLUSTERING ORDER` option is
not used) is strictly equivalent to using the option with all clustering
columns using the `ASC` modifier.

Note that this option is basically a hint for the storage engine to
change the order in which it stores the row but it has 3 visible
consequences:

* it limits which `ORDER BY` clause are allowed for
`selects <select-statement>` on that table. You can only::
  order results by the clustering order or the reverse clustering order.
  Meaning that if a table has 2 clustering column `a` and `b` and you
  defined `WITH CLUSTERING ORDER (a DESC, b ASC)`, then in queries you
  will be allowed to use `ORDER BY (a DESC, b ASC)` and (reverse
  clustering order) `ORDER BY (a ASC, b DESC)` but *not*
  `ORDER BY (a ASC, b ASC)` (nor `ORDER BY (a DESC, b DESC)`).
* it also change the default order of results when queried (if no
`ORDER BY` is provided). Results are always returned::
  in clustering order (within a partition).
* it has a small performance impact on some queries as queries in
reverse clustering order are slower than the one in::
  forward clustering order. In practice, this means that if you plan on
  querying mostly in the reverse natural order of your columns (which is
  common with time series for instance where you often want data from
  the newest to the oldest), it is an optimization to declare a
  descending clustering order.

[[create-table-general-options]]
==== Other table options

review (misses cdc if nothing else) and link to proper categories when
appropriate (compaction for instance)

A table supports the following options:

[width="100%",cols="30%,9%,11%,50%",options="header",]
|===
|option | kind | default | description

| `comment` | _simple_ | none | A free-form, human-readable comment. 
| `speculative_retry` | _simple_ | 99PERCENTILE | Speculative retry options
| `cdc` |_boolean_ |false |Create a Change Data Capture (CDC) log on the table.
| `additional_write_policy` |_simple_ |99PERCENTILE |`Speculative retry options
| `gc_grace_seconds` |_simple_ |864000 |Time to wait before garbage
collecting tombstones (deletion markers).
| `bloom_filter_fp_chance` |_simple_ |0.00075 |The target probability of
false positive of the sstable bloom filters. Said bloom filters will be
sized to provide the provided probability (thus lowering this value
impact the size of bloom filters in-memory and on-disk)
| `default_time_to_live` |_simple_ |0 |The default expiration time (“TTL”) in seconds for a table.
| `compaction` |_map_ |_see below_ |`Compaction options <cql-compaction-options>`.
| `compression` |_map_ |_see below_ |`Compression options <cql-compression-options>`.
| `caching` |_map_ |_see below_ |Caching options
| `memtable_flush_period_in_ms` |_simple_ |0 |Time (in ms) before Cassandra flushes memtables to disk.
| `read_repair` |_simple_ |BLOCKING |Sets read repair behavior (see below)
|===

===== Speculative retry options

By default, Cassandra read coordinators only query as many replicas as
necessary to satisfy consistency levels: one for consistency level
`ONE`, a quorum for `QUORUM`, and so on. `speculative_retry` determines
when coordinators may query additional replicas, which is useful when
replicas are slow or unresponsive. Speculative retries are used to
reduce the latency. The speculative_retry option may be used to
configure rapid read protection with which a coordinator sends more
requests than needed to satisfy the Consistency level.

Pre-4.0 speculative Retry Policy takes a single string as a parameter,
this can be `NONE`, `ALWAYS`, `99PERCENTILE` (PERCENTILE), `50MS`
(CUSTOM).

Examples of setting speculative retry are:

[source,cql]
----
ALTER TABLE users WITH speculative_retry = '10ms';
----

Or,

[source,cql]
----
ALTER TABLE users WITH speculative_retry = '99PERCENTILE';
----

The problem with these settings is when a single host goes into an
unavailable state this drags up the percentiles. This means if we are
set to use `p99` alone, we might not speculate when we intended to to
because the value at the specified percentile has gone so high. As a fix
4.0 adds support for hybrid `MIN()`, `MAX()` speculative retry policies
(https://issues.apache.org/jira/browse/CASSANDRA-14293[CASSANDRA-14293]).
This means if the normal `p99` for the table is <50ms, we will still
speculate at this value and not drag the tail latencies up... but if the
`p99th` goes above what we know we should never exceed we use that
instead.

In 4.0 the values (case-insensitive) discussed in the following table
are supported:

[cols=",,",options="header",]
|===
|Format |Example |Description
| `XPERCENTILE` | 90.5PERCENTILE | Coordinators record average per-table response times
for all replicas. If a replica takes longer than `X` percent of this
table's average response time, the coordinator queries an additional
replica. `X` must be between 0 and 100.
| `XP` | 90.5P | Synonym for `XPERCENTILE` 
| `Yms` | 25ms | If a replica takes more than `Y` milliseconds to respond, the
coordinator queries an additional replica.
| `MIN(XPERCENTILE,YMS)` | MIN(99PERCENTILE,35MS) | A hybrid policy that will use either the
specified percentile or fixed milliseconds depending on which value is
lower at the time of calculation. Parameters are `XPERCENTILE`, `XP`, or
`Yms`. This is helpful to help protect against a single slow instance;
in the happy case the 99th percentile is normally lower than the
specified fixed value however, a slow host may skew the percentile very
high meaning the slower the cluster gets, the higher the value of the
percentile, and the higher the calculated time used to determine if we
should speculate or not. This allows us to set an upper limit that we
want to speculate at, but avoid skewing the tail latencies by
speculating at the lower value when the percentile is less than the
specified fixed upper bound.

| `MAX(XPERCENTILE,YMS)` `ALWAYS` `NEVER` | MAX(90.5P,25ms) | A hybrid policy that will use either the specified
percentile or fixed milliseconds depending on which value is higher at
the time of calculation. Coordinators always query all replicas.
Coordinators never query additional replicas.
|===

As of version 4.0 speculative retry allows more friendly params
(https://issues.apache.org/jira/browse/CASSANDRA-13876[CASSANDRA-13876]).
The `speculative_retry` is more flexible with case. As an example a
value does not have to be `NONE`, and the following are supported
alternatives.

[source,cql]
----
alter table users WITH speculative_retry = 'none';
alter table users WITH speculative_retry = 'None';
----

The text component is case insensitive and for `nPERCENTILE` version 4.0
allows `nP`, for instance `99p`. In a hybrid value for speculative
retry, one of the two values must be a fixed millisecond value and the
other a percentile value.

Some examples:

[source,cql]
----
min(99percentile,50ms)
max(99p,50MS)
MAX(99P,50ms)
MIN(99.9PERCENTILE,50ms)
max(90percentile,100MS)
MAX(100.0PERCENTILE,60ms)
----

Two values of the same kind cannot be specified such as
`min(90percentile,99percentile)` as it wouldn’t be a hybrid value. This
setting does not affect reads with consistency level `ALL` because they
already query all replicas.

Note that frequently reading from additional replicas can hurt cluster
performance. When in doubt, keep the default `99PERCENTILE`.

`additional_write_policy` specifies the threshold at which a cheap
quorum write will be upgraded to include transient replicas.

[[cql-compaction-options]]
===== Compaction options

The `compaction` options must at least define the `'class'` sub-option,
that defines the compaction strategy class to use. The supported class
are `'SizeTieredCompactionStrategy'` (`STCS <STCS>`),
`'LeveledCompactionStrategy'` (`LCS <LCS>`) and
`'TimeWindowCompactionStrategy'` (`TWCS <TWCS>`) (the
`'DateTieredCompactionStrategy'` is also supported but is deprecated and
`'TimeWindowCompactionStrategy'` should be preferred instead). The
default is `'SizeTieredCompactionStrategy'`. Custom strategy can be
provided by specifying the full class name as a `string constant
<constants>`.

All default strategies support a number of
`common options <compaction-options>`, as well as options specific to
the strategy chosen (see the section corresponding to your strategy for
details: `STCS <stcs-options>`, `LCS
<lcs-options>` and `TWCS <TWCS>`).

[[cql-compression-options]]
===== Compression options

The `compression` options define if and how the sstables of the table
are compressed. Compression is configured on a per-table basis as an
optional argument to `CREATE TABLE` or `ALTER TABLE`. The following
sub-options are available:

[cols=",,",options="header",]
|===
|Option |Default |Description

| `class` | LZ4Compressor | The compression algorithm to use. Default compressor are: LZ4Compressor,
SnappyCompressor, DeflateCompressor and ZstdCompressor. Use
`'enabled' : false` to disable compression. Custom compressor can be
provided by specifying the full class name as a “string constant”:constants.

| `enabled` | true | Enable/disable sstable compression. If the `enabled` option is set to
`false` no other options must be specified.

| `chunk_length_in_kb` | 64 | On disk SSTables are compressed by block (to allow random reads). This
defines the size (in KB) of said block. Bigger values may improve the
compression rate, but increases the minimum size of data to be read from
disk for a read. The default value is an optimal value for compressing
tables. Chunk length must be a power of 2 because so is assumed so when
computing the chunk number from an uncompressed file offset. Block size
may be adjusted based on read/write access patterns such as:

* How much data is typically requested at once
* Average size of rows in the table

| `crc_check_chance` | 1.0 | Determines how likely Cassandra is to verify the checksum on each
compression chunk during reads.

| `compression_level` | 3 | Compression level. It is only applicable for `ZstdCompressor` and
accepts values between `-131072` and `22`.
|===

For instance, to create a table with LZ4Compressor and a
chunk_lenth_in_kb of 4KB:

[source,cql]
----
CREATE TABLE simple (
   id int,
   key text,
   value text,
   PRIMARY KEY (key, value)
) with compression = {'class': 'LZ4Compressor', 'chunk_length_in_kb': 4};
----

[[cql-caching-options]]
===== Caching options

Caching optimizes the use of cache memory of a table. The cached data is
weighed by size and access frequency. The `caching` options allows to
configure both the _key cache_ and the _row cache_ for the table. The
following sub-options are available:

[cols=",,",options="header",]
|===
|Option |Default |Description
a|
____
`keys`
____

a|
____
ALL
____

a|
____
Whether to cache keys (“key cache”) for this table. Valid values are:
`ALL` and `NONE`.
____

a|
____
`rows_per_partition`
____

a|
____
NONE
____

a|
____
The amount of rows to cache per partition (“row cache”). If an integer
`n` is specified, the first `n` queried rows of a partition will be
cached. Other possible options are `ALL`, to cache all rows of a queried
partition, or `NONE` to disable row caching.
____

|===

For instance, to create a table with both a key cache and 10 rows per
partition:

[source,cql]
----
CREATE TABLE simple (
id int,
key text,
value text,
PRIMARY KEY (key, value)
) WITH caching = {'keys': 'ALL', 'rows_per_partition': 10};
----

===== Read Repair options

The `read_repair` options configures the read repair behavior to allow
tuning for various performance and consistency behaviors. Two
consistency properties are affected by read repair behavior.

* Monotonic Quorum Reads: Provided by `BLOCKING`. Monotonic quorum reads
prevents reads from appearing to go back in time in some circumstances.
When monotonic quorum reads are not provided and a write fails to reach
a quorum of replicas, it may be visible in one read, and then disappear
in a subsequent read.
* Write Atomicity: Provided by `NONE`. Write atomicity prevents reads
from returning partially applied writes. Cassandra attempts to provide
partition level write atomicity, but since only the data covered by a
SELECT statement is repaired by a read repair, read repair can break
write atomicity when data is read at a more granular level than it is
written. For example read repair can break write atomicity if you write
multiple rows to a clustered partition in a batch, but then select a
single row by specifying the clustering column in a SELECT statement.

The available read repair settings are:

==== Blocking

The default setting. When `read_repair` is set to `BLOCKING`, and a read
repair is triggered, the read will block on writes sent to other
replicas until the CL is reached by the writes. Provides monotonic
quorum reads, but not partition level write atomicity

==== None

When `read_repair` is set to `NONE`, the coordinator will reconcile any
differences between replicas, but will not attempt to repair them.
Provides partition level write atomicity, but not monotonic quorum
reads.

===== Other considerations:

* Adding new columns (see `ALTER TABLE` below) is a constant time
operation. There is thus no need to try to anticipate future usage when
creating a table.

[[alter-table-statement]]
== ALTER TABLE

Altering an existing table uses the `ALTER TABLE` statement:

alter_table_statement: ALTER TABLE table_name
alter_table_instruction alter_table_instruction: ADD
column_name cql_type ( ','
column_name cql_type )* : | DROP
column_name ( column_name )* : | WITH
options

For instance:

[source,cql]
----
ALTER TABLE addamsFamily ADD gravesite varchar;

ALTER TABLE addamsFamily
       WITH comment = 'A most excellent and useful table';
----

The `ALTER TABLE` statement can:

* Add new column(s) to the table (through the `ADD` instruction). Note
that the primary key of a table cannot be changed and thus newly added
column will, by extension, never be part of the primary key. Also note
that `compact
tables <compact-tables>` have restrictions regarding column addition.
Note that this is constant (in the amount of data the cluster contains)
time operation.
* Remove column(s) from the table. This drops both the column and all
its content, but note that while the column becomes immediately
unavailable, its content is only removed lazily during compaction.
Please also see the warnings below. Due to lazy removal, the altering
itself is a constant (in the amount of data removed or contained in the
cluster) time operation.
* Change some of the table options (through the `WITH` instruction). The
`supported options
<create-table-options>` are the same that when creating a table (outside
of `COMPACT STORAGE` and `CLUSTERING ORDER` that cannot be changed after
creation). Note that setting any `compaction` sub-options has the effect
of erasing all previous `compaction` options, so you need to re-specify
all the sub-options if you want to keep them. The same note applies to
the set of `compression` sub-options.

[WARNING]
.Warning
====
Dropping a column assumes that the timestamps used for the value of this
column are "real" timestamp in microseconds. Using "real" timestamps in
microseconds is the default is and is *strongly* recommended but as
Cassandra allows the client to provide any timestamp on any table it is
theoretically possible to use another convention. Please be aware that
if you do so, dropping a column will not work correctly.
====

[WARNING]
.Warning
====
Once a column is dropped, it is allowed to re-add a column with the same
name than the dropped one *unless* the type of the dropped column was a
(non-frozen) column (due to an internal technical limitation).
====

[[drop-table-statement]]
== DROP TABLE

Dropping a table uses the `DROP TABLE` statement:

drop_table_statement: DROP TABLE [ IF EXISTS ] table_name

Dropping a table results in the immediate, irreversible removal of the
table, including all data it contains.

If the table does not exist, the statement will return an error, unless
`IF EXISTS` is used in which case the operation is a no-op.

[[truncate-statement]]
== TRUNCATE

A table can be truncated using the `TRUNCATE` statement:

truncate_statement: TRUNCATE [ TABLE ] table_name

Note that `TRUNCATE TABLE foo` is allowed for consistency with other DDL
statements but tables are the only object that can be truncated
currently and so the `TABLE` keyword can be omitted.

Truncating a table permanently removes all existing data from the table,
but without removing the table itself.
